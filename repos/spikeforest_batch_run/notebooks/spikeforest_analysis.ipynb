{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "spikeforest_analysis.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "blWcqWOIhzkc",
        "rvCvUJ1JiZX1",
        "TKwgGlPKh5uE",
        "1vvFYgXjmetF",
        "dfDRhV7jpVgy",
        "fL2QjjBPqXS7"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/magland/spikeforest_batch_run/blob/master/notebooks/spikeforest_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "xfQAJFPFgbzE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# SpikeForest analysis"
      ]
    },
    {
      "metadata": {
        "id": "Fcx4kQoVt374",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This notebook represents a complete spikeforest analysis of the bionet studies. You should execute the first few cells and then skip down to the section of interest below."
      ]
    },
    {
      "metadata": {
        "id": "vtsE5P3EgXYr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Only run this cell if you are running this on a hosted runtime that does not have these packages installed\n",
        "# %%capture is used to suppress the output... this should take up to a minute to complete\n",
        "%%capture\n",
        "!pip install spikeforest\n",
        "!pip install git+https://github.com/magland/spikeforest_batch_run"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "P3-FpNWUgWln",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Import the python packages -- autoreload is used for development purposes\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import spikeforest as sf\n",
        "from kbucket import client as kb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_JHpizoTul8M",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Configure readonly access to kbucket -- use this if you only want to browse the results ---\n",
        "sf.kbucketConfigRemote(name='spikeforest1-readonly')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3pHRiMtggkaT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Configure read/write access to kbucket -- use this if you are preparing the studies or the processing batches\n",
        "sf.kbucketConfigRemote(name='spikeforest1-readwrite',ask_password=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "blWcqWOIhzkc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Prepare recordings"
      ]
    },
    {
      "metadata": {
        "id": "HtbjJUirgm0s",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def read_text_file(path):\n",
        "  path2=kb.realizeFile(path)\n",
        "  if path2 is None:\n",
        "    raise Exception('Unable to realize file: '+path)\n",
        "  with open(path2,'r') as f:\n",
        "    return f.read()\n",
        "  \n",
        "def prepare_bionet_studies(*,basedir,channels):\n",
        "  study_set_name='bionet'\n",
        "  studies=[]\n",
        "  recordings=[]\n",
        "  names=['bionet_drift','bionet_shuffle','bionet_static']\n",
        "  for name in names:\n",
        "    study_name=name\n",
        "    study_dir=basedir+'/bionet/'+name\n",
        "    description=read_text_file(study_dir+'/readme.txt')\n",
        "    study0=dict(\n",
        "        name=study_name,\n",
        "        study_set=study_set_name,\n",
        "        directory=study_dir,\n",
        "        description=description\n",
        "    )\n",
        "    studies.append(study0)\n",
        "    dd=kb.readDir(study_dir)\n",
        "    for dsname in dd['dirs']:\n",
        "        dsdir='{}/{}'.format(study_dir,dsname)\n",
        "        rec0=dict(\n",
        "            name=dsname,\n",
        "            study=study_name,\n",
        "            description='',\n",
        "            directory=dsdir,\n",
        "            channels=channels\n",
        "        )\n",
        "        if len(rec0['channels'])>0:\n",
        "          units=sf.sf_batch.select_units_on_channels(\n",
        "              recording_dir=dsdir,\n",
        "              firings=dsdir+'/firings_true.mda',\n",
        "              channels=rec0['channels']\n",
        "          )\n",
        "          rec0['units_true']=units\n",
        "        recordings.append(rec0)\n",
        "  return studies, recordings\n",
        "\n",
        "def prepare_magland_synth_studies(*,basedir):\n",
        "  study_set_name='magland_synth'\n",
        "  studies=[]\n",
        "  recordings=[]\n",
        "  names=[]\n",
        "  names=names+['datasets_noise10_K10_C4','datasets_noise10_K10_C8']\n",
        "  names=names+['datasets_noise10_K20_C4','datasets_noise10_K20_C8']\n",
        "  names=names+['datasets_noise20_K10_C4','datasets_noise20_K10_C8']\n",
        "  names=names+['datasets_noise20_K20_C4','datasets_noise20_K20_C8']\n",
        "  description=read_text_file(basedir+'/magland_synth/readme.txt')\n",
        "  for name in names:\n",
        "    study_name='magland_synth_'+name[9:]\n",
        "    study_dir=basedir+'/magland_synth/'+name\n",
        "    study0=dict(\n",
        "        name=study_name,\n",
        "        study_set=study_set_name,\n",
        "        directory=study_dir,\n",
        "        description=description\n",
        "    )\n",
        "    studies.append(study0)\n",
        "    dd=kb.readDir(study_dir)\n",
        "    for dsname in dd['dirs']:\n",
        "        dsdir='{}/{}'.format(study_dir,dsname)\n",
        "        recordings.append(dict(\n",
        "            name=dsname,\n",
        "            study=study_name,\n",
        "            directory=dsdir,\n",
        "            description='One of the recordings in the {} study'.format(study_name)\n",
        "        ))\n",
        "  return studies, recordings\n",
        "\n",
        "def prepare_mearec_tetrode_studies(*,basedir):\n",
        "  study_set_name='mearec_tetrode'\n",
        "  studies=[]\n",
        "  recordings=[]\n",
        "  names=[]\n",
        "  names=names+['datasets_noise10_K10_C4','datasets_noise10_K20_C4']\n",
        "  names=names+['datasets_noise20_K10_C4','datasets_noise20_K20_C4']\n",
        "  description=read_text_file(basedir+'/mearec_synth/readme.txt')\n",
        "  for name in names:\n",
        "    study_name='mearec_tetrode_'+name[9:]\n",
        "    study_dir=basedir+'/mearec_synth/tetrode/'+name\n",
        "    study0=dict(\n",
        "        name=study_name,\n",
        "        study_set=study_set_name,\n",
        "        directory=study_dir,\n",
        "        description=description\n",
        "    )\n",
        "    studies.append(study0)\n",
        "    dd=kb.readDir(study_dir)\n",
        "    for dsname in dd['dirs']:\n",
        "        dsdir='{}/{}'.format(study_dir,dsname)\n",
        "        recordings.append(dict(\n",
        "            name=dsname,\n",
        "            study=study_name,\n",
        "            directory=dsdir,\n",
        "            description='One of the recordings in the {} study'.format(study_name)\n",
        "        ))\n",
        "  return studies, recordings"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BpXkEoS8gaMn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "basedir='kbucket://15734439d8cf/groundtruth'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "b92joharg0G5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "channels=[0,1,2,3,4,5,6,7]\n",
        "studies,recordings=prepare_bionet_studies(basedir=basedir,channels=channels)\n",
        "kb.saveObject(dict(studies=studies,recordings=recordings),key=dict(name='spikeforest_bionet_recordings'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XCUMPH8JgXK7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "studies,recordings=prepare_magland_synth_studies(basedir=basedir)\n",
        "kb.saveObject(dict(studies=studies,recordings=recordings),key=dict(name='spikeforest_magland_synth_recordings'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rvCvUJ1JiZX1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Create summarize recordings batches"
      ]
    },
    {
      "metadata": {
        "id": "MWIJU5Gxic_f",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_summarize_recordings_batch(*,recordings_name,batch_name):\n",
        "  print('Creating summarize_recordings batch: '+batch_name)\n",
        "  SF=sf.SFData()\n",
        "  SF.loadRecordings(key=dict(name=recordings_name))\n",
        "  \n",
        "  jobs=[]\n",
        "  for name in SF.studyNames():\n",
        "    study=SF.study(name)\n",
        "    for recname in study.recordingNames():\n",
        "      R=study.recording(recname)\n",
        "      job=dict(\n",
        "          command='summarize_recording',\n",
        "          label=R.name(),\n",
        "          recording=R.getObject()\n",
        "      )\n",
        "      jobs.append(job)\n",
        "  batch=dict(jobs=jobs)\n",
        "  print('Number of jobs: {}'.format(len(jobs)))\n",
        "  kb.saveObject(key=dict(batch_name=batch_name),object=batch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Vrw88QA5hACU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "create_summarize_recordings_batch(recordings_name='spikeforest_bionet_recordings',batch_name='summarize_recordings_bionet')\n",
        "create_summarize_recordings_batch(recordings_name='spikeforest_magland_synth_recordings',batch_name='summarize_recordings_magland_synth')\n",
        "create_summarize_recordings_batch(recordings_name='spikeforest_mearec_tetrode_recordings',batch_name='summarize_recordings_mearec_tetrode')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "weikHp2gjbwJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "To run these batches, go to a computer with resources somewhere and run something like:\n",
        "\n",
        "```\n",
        "bin/sf_run_batch [name_of_batch] --run_prefix \"srun -c 2 -n 40\"\n",
        "```\n",
        "\n",
        "\"srun\" in Flatiron cluster reqiures you to run the following before\n",
        "```\n",
        "module load srun\n",
        "module load matlab\n",
        "```\n",
        "To use GPU, run\n",
        "```\n",
        "bin/sf_run_batch [name_of_batch] --run_prefix \"srun -c 2 -n 40 --gres=gpu:2 -p gpu\"\n",
        "```\n",
        "  \n",
        "where bin/sf_run_batch is found in the spikeforest_batch_run repository.\n",
        "\n",
        "Alternatively, you can test run it in this notebook using the following commands:\n"
      ]
    },
    {
      "metadata": {
        "id": "AivwnHWOMZvx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "To clear previous batch result, run\n",
        "```\n",
        "bin/sf_run_batch_command clear [name_of_batch]\n",
        "```\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "z5k_8AHGk0mu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Note: usually you would not run this cell -- see the note above.\n",
        "\n",
        "import spikeforest_batch_run as sbr\n",
        "# Execute prepareBatch once (serially)\n",
        "sbr.prepareBatch(batch_name='summarize_recordings_bionet')\n",
        "\n",
        "# Execute runBatch many times in parallel\n",
        "sbr.runBatch(batch_name='summarize_recordings_bionet')\n",
        "\n",
        "# Execute assembleBatchResults once (serially)\n",
        "sbr.assembleBatchResults(batch_name='summarize_recordings_bionet')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TKwgGlPKh5uE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Browse recordings"
      ]
    },
    {
      "metadata": {
        "id": "-qq56y38hJpv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "SF=sf.SFData()\n",
        "SF.loadRecordings(key=dict(name='spikeforest_bionet_recordings'))\n",
        "SF.loadRecordings(key=dict(name='spikeforest_magland_synth_recordings'))\n",
        "SF.loadProcessingBatch(key=dict(batch_name='summarize_recordings_bionet',name='job_results'))\n",
        "SF.loadProcessingBatch(key=dict(batch_name='summarize_recordings_magland_synth',name='job_results'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6o8vClJ9hrRa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X=sf.SFSelectWidget(sfdata=SF,mode='recording')\n",
        "display(X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CwiDbAZbiF48",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "R=X.recording()\n",
        "display(R.plot('timeseries'))\n",
        "display(R.plot('waveforms_true'))\n",
        "display(R.trueUnitsInfo())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WBdqbkSNiJyS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "R.plotNames()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1vvFYgXjmetF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Create spike sorting batches"
      ]
    },
    {
      "metadata": {
        "id": "7toleMsBnMy6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "SF=sf.SFData()\n",
        "SF.loadRecordings(key=dict(name='spikeforest_bionet_recordings'))\n",
        "SF.loadRecordings(key=dict(name='spikeforest_magland_synth_recordings'))\n",
        "SF.loadProcessingBatch(key=dict(batch_name='summarize_recordings_bionet',name='job_results'))\n",
        "SF.loadProcessingBatch(key=dict(batch_name='summarize_recordings_magland_synth',name='job_results'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2PHEdeuBnVJl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sorter_ms4_thr3=dict(\n",
        "    name='MountainSort4-thr3',\n",
        "    processor_name='MountainSort4',\n",
        "    params=dict(\n",
        "        detect_sign=-1,\n",
        "        adjacency_radius=100,\n",
        "        detect_threshold=3\n",
        "    )\n",
        ")\n",
        "\n",
        "sorter_irc_tetrode=dict(\n",
        "    name='IronClust-tetrode',\n",
        "    processor_name='IronClust',\n",
        "    params=dict(\n",
        "        detect_sign=-1,\n",
        "        adjacency_radius=100,\n",
        "        detect_threshold=5,\n",
        "        prm_template_name=\"tetrode_template.prm\"\n",
        "    )\n",
        ")\n",
        "\n",
        "sorter_irc_drift=dict(\n",
        "    name='IronClust-drift',\n",
        "    processor_name='IronClust',\n",
        "    params=dict(\n",
        "        detect_sign=-1,\n",
        "        adjacency_radius=100,\n",
        "        prm_template_name=\"template_drift.prm\"\n",
        "    )\n",
        ")\n",
        "\n",
        "sorter_sc=dict(\n",
        "    name='SpykingCircus',\n",
        "    processor_name='SpykingCircus',\n",
        "    params=dict(\n",
        "        detect_sign=-1,\n",
        "        adjacency_radius=100\n",
        "    )\n",
        ")\n",
        "\n",
        "sorter_ks_tetrode=dict(\n",
        "    name='KiloSort',\n",
        "    processor_name='KiloSort',\n",
        "    params=dict(\n",
        "        detect_sign=-1,\n",
        "        adjacency_radius=-1\n",
        "    )\n",
        ")\n",
        "\n",
        "sorter_ks_drift=dict(\n",
        "    name='KiloSort',\n",
        "    processor_name='KiloSort',\n",
        "    params=dict(\n",
        "        detect_sign=-1,\n",
        "        adjacency_radius=100\n",
        "    )\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NhvWMXK7mSYu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_sorting_batch(*,recordings_name,batch_name,sorters):\n",
        "  print('Creating sorting batch: '+batch_name)\n",
        "  SF=sf.SFData()\n",
        "  SF.loadRecordings(key=dict(name=recordings_name))\n",
        "  \n",
        "  jobs=[]\n",
        "  for name in SF.studyNames():\n",
        "    study=SF.study(name)\n",
        "    for rname in study.recordingNames():\n",
        "      R=study.recording(rname)\n",
        "      for sorter in sorters:\n",
        "        job=dict(\n",
        "          command='sort_recording',\n",
        "          label=sorter['name']+': '+R.name(),\n",
        "          recording=R.getObject(),\n",
        "          sorter=sorter\n",
        "        )\n",
        "        jobs.append(job)\n",
        "\n",
        "  batch=dict(jobs=jobs)\n",
        "  print('Number of jobs: {}'.format(len(jobs)))\n",
        "  kb.saveObject(key=dict(batch_name=batch_name),object=batch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yb-zTMQgnK98",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "create_sorting_batch(recordings_name='spikeforest_magland_synth_recordings',batch_name='ms4_magland_synth',sorters=[sorter_ms4_thr3])\n",
        "create_sorting_batch(recordings_name='spikeforest_magland_synth_recordings',batch_name='irc_magland_synth',sorters=[sorter_irc_tetrode])\n",
        "create_sorting_batch(recordings_name='spikeforest_magland_synth_recordings',batch_name='sc_magland_synth',sorters=[sorter_sc])\n",
        "create_sorting_batch(recordings_name='spikeforest_magland_synth_recordings',batch_name='ks_magland_synth',sorters=[sorter_ks_tetrode])\n",
        "\n",
        "create_sorting_batch(recordings_name='spikeforest_bionet_recordings',batch_name='ms4_bionet',sorters=[sorter_ms4_thr3])\n",
        "create_sorting_batch(recordings_name='spikeforest_bionet_recordings',batch_name='irc_bionet',sorters=[sorter_irc_drift])\n",
        "create_sorting_batch(recordings_name='spikeforest_bionet_recordings',batch_name='sc_bionet',sorters=[sorter_sc])\n",
        "create_sorting_batch(recordings_name='spikeforest_bionet_recordings',batch_name='ks_bionet',sorters=[sorter_ks_drift])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W2VTozxFnoE3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "To run these sorting batches, follow the instructions above."
      ]
    },
    {
      "metadata": {
        "id": "dfDRhV7jpVgy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Browse sorting results"
      ]
    },
    {
      "metadata": {
        "id": "4ga6PepCpXKF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "SF=sf.SFData()\n",
        "SF.loadRecordings(key=dict(name='spikeforest_bionet_recordings'))\n",
        "SF.loadRecordings(key=dict(name='spikeforest_magland_synth_recordings'))\n",
        "SF.loadRecordings(key=dict(name='spikeforest_mearec_tetrode_recordings'))\n",
        "\n",
        "SF.loadProcessingBatch(key=dict(batch_name='summarize_recordings_bionet',name='job_results'))\n",
        "SF.loadProcessingBatch(key=dict(batch_name='summarize_recordings_magland_synth',name='job_results'))\n",
        "SF.loadProcessingBatch(key=dict(batch_name='summarize_recordings_mearec_tetrode',name='job_results'))\n",
        "\n",
        "SF.loadProcessingBatch(key=dict(batch_name='ms4_magland_synth',name='job_results'))\n",
        "SF.loadProcessingBatch(key=dict(batch_name='sc_magland_synth',name='job_results'))\n",
        "SF.loadProcessingBatch(key=dict(batch_name='irc_magland_synth',name='job_results'))\n",
        "SF.loadProcessingBatch(key=dict(batch_name='ks_magland_synth',name='job_results'))\n",
        "\n",
        "SF.loadProcessingBatch(key=dict(batch_name='ms4_mearec_tetrode',name='job_results'))\n",
        "#SF.loadProcessingBatch(key=dict(batch_name='sc_mearec_tetrode',name='job_results'))\n",
        "SF.loadProcessingBatch(key=dict(batch_name='irc_mearec_tetrode',name='job_results'))\n",
        "SF.loadProcessingBatch(key=dict(batch_name='ks_mearec_tetrode',name='job_results'))\n",
        "\n",
        "SF.loadProcessingBatch(key=dict(batch_name='ms4_bionet',name='job_results'))\n",
        "SF.loadProcessingBatch(key=dict(batch_name='sc_bionet',name='job_results')) ## Spyking circus not working yet -- need to put into singularity container\n",
        "SF.loadProcessingBatch(key=dict(batch_name='irc_bionet',name='job_results'))\n",
        "#SF.loadProcessingBatch(key=dict(batch_name='ks_bionet',name='job_results'))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D7Ilcyb0pki5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X=sf.SFSelectWidget(sfdata=SF,mode='sorting_result')\n",
        "display(X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Sganjg9cp56c",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "R=X.sortingResult()\n",
        "display(R.plot('unit_waveforms'))\n",
        "display(R.plot('autocorrelograms'))\n",
        "display(R.comparisonWithTruth())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fL2QjjBPqXS7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Aggregate sorting results"
      ]
    },
    {
      "metadata": {
        "id": "bBl4NBQCrXl3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "SF=sf.SFData()\n",
        "SF.loadRecordings(key=dict(name='spikeforest_bionet_recordings'))\n",
        "SF.loadProcessingBatch(key=dict(batch_name='summarize_recordings_bionet',name='job_results'))\n",
        "SF.loadProcessingBatch(key=dict(batch_name='ms4_bionet',name='job_results'))\n",
        "#SF.loadProcessingBatch(key=dict(batch_name='sc_bionet',name='job_results')) ## Spyking circus not working yet -- need to put into singularity container\n",
        "SF.loadProcessingBatch(key=dict(batch_name='irc_bionet',name='job_results'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XyBRKILPp-0h",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "import altair as alt\n",
        "alt.renderers.enable('colab')\n",
        "\n",
        "# Accumulate the sorting results\n",
        "def accumulate_comparison_with_ground_truth(*,SF,studies,sorter_name,fieldnames):\n",
        "  ret=[]\n",
        "  for study in studies:\n",
        "    recordings=[study.recording(name) for name in study.recordingNames()]\n",
        "    for R in recordings:\n",
        "      result=R.sortingResult(sorter_name)\n",
        "      A=result.comparisonWithTruth(format='json')\n",
        "      B=R.trueUnitsInfo(format='json')\n",
        "      snr_by_true_unit=dict()\n",
        "      for b in B:\n",
        "        snr_by_true_unit[b['unit_id']]=b['snr']\n",
        "      for i in A:\n",
        "        a=A[i]\n",
        "        rec=dict()\n",
        "        rec['recording_name']=R.name()\n",
        "        rec['unit_id']=a['Unit ID']\n",
        "        rec['snr']=snr_by_true_unit[rec['unit_id']]\n",
        "        for fieldname in fieldnames:\n",
        "          rec[fieldname]=float(a[fieldname])\n",
        "        ret.append(rec)\n",
        "  return ret\n",
        "\n",
        "def show_accuracy_plot(*,SF,study_name,sorter_name,title):\n",
        "  \n",
        "  study=SF.study(study_name)\n",
        "  X=accumulate_comparison_with_ground_truth(\n",
        "      SF=SF,\n",
        "      studies=[study],\n",
        "      sorter_name=sorter_name,\n",
        "      fieldnames=['Accuracy']\n",
        "  )\n",
        "  \n",
        "  # Display the accumulated sorting results\n",
        "  cc=alt.Chart(pd.DataFrame(X),title=title).mark_point().encode(\n",
        "      x='snr',\n",
        "      y='Accuracy',\n",
        "      color='recording_name',\n",
        "      tooltip='recording_name'\n",
        "  ).interactive()\n",
        "  display(cc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8-bIDoH1rkYW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import vdomr as vd\n",
        "\n",
        "class SelectBox(vd.Component):\n",
        "    def __init__(self,options=[]):\n",
        "        vd.Component.__init__(self)\n",
        "        self._on_change_handlers=[]\n",
        "        self._value=None\n",
        "        self.setOptions(options)\n",
        "        \n",
        "    def setOptions(self,options):\n",
        "        self._options=options\n",
        "        if self._value not in options:\n",
        "          self._value=options[0] if options else None\n",
        "        self.refresh()\n",
        "        \n",
        "    def value(self):\n",
        "        return self._value\n",
        "    \n",
        "    def setValue(self,value):\n",
        "        self._value=value\n",
        "        self.refresh()\n",
        "        \n",
        "    def onChange(self,handler):\n",
        "        self._on_change_handlers.append(handler)\n",
        "        \n",
        "    def _on_change(self,value):\n",
        "        self._value=value\n",
        "        for handler in self._on_change_handlers:\n",
        "            handler(value=value)\n",
        "        \n",
        "    def render(self):\n",
        "        opts=[]\n",
        "        for option in self._options:\n",
        "            if option==self._value:\n",
        "              opts.append(vd.option(option,selected='selected'))\n",
        "            else:\n",
        "              opts.append(vd.option(option))\n",
        "        X=vd.select(opts,onchange=self._on_change)\n",
        "        return X"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "THJyQ4V4rGT6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "STUDY=SelectBox(options=SF.studyNames())\n",
        "SORTER=SelectBox(options=['MountainSort4-thr3','IronClust-drift'])\n",
        "display(STUDY)\n",
        "display(SORTER)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RtlT2PGGJWgz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(SORTER.value())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fI6RItt-q5pM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "show_accuracy_plot(\n",
        "    SF=SF,\n",
        "    study_name=STUDY.value(),\n",
        "    sorter_name=SORTER.value(),\n",
        "    title=SORTER.value()+' '+STUDY.value()\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Opz2K0VPrZAm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import output\n",
        "print(output.__file__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "v7tB7Iq9GwMh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}