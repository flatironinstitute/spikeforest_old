#!/usr/bin/env python

from mountaintools import client as mt
import multiprocessing
import os
import sys
import shutil
import json
import random
import mlprocessors as mlpr
import mtlogging
import argparse
from copy import deepcopy
import spikeforest_analysis as sa
import numpy as np
from typing import List, Optional


@mtlogging.log(root=True)
def main():

    parser = argparse.ArgumentParser(description='Run the SpikeForest main analysis')
    parser.add_argument('analysis_file', help='Path to the analysis specification file (.json format).')
    parser.add_argument('--analyses', help='Comma-separated names of analyses to run', required=True)
    parser.add_argument('--job_timeout', help='Job timeout in seconds. Default is 20 minutes = 1200 seconds.', required=False, default=1200)
    parser.add_argument('--skip_failing', help='Use cached version of failing processes.', action='store_true')
    parser.add_argument('--skip_timed_out', help='Use cached version of timed out processes.', action='store_true')
    parser.add_argument('--use_slurm', help='Whether to use slurm.', action='store_true')
    parser.add_argument('--verbose', help='Provide some additional verbose output.', action='store_true')
    parser.add_argument('--test', help='Only run a few, and do not save output.', action='store_true')
    parser.add_argument('--upload_to', help='Upload destination(s) separated by comma', default=None, required=False)
    parser.add_argument('--download_from', help='Download destination(s) separated by comma', default=None, required=False)

    args = parser.parse_args()
    print(args)

    upload_to: Optional[List[str]] = None
    if args.upload_to:
        upload_to = args.upload_to.split(',')

    download_from: Optional[List[str]] = None
    if args.download_from:
        download_from = args.download_from.split(',')
        
    mt.configDownloadFrom(download_from)

    # mt.configDownloadFrom(['spikeforest.kbucket', 'spikeforest.public'])
    # upload_to = ['spikeforest.kbucket', 'spikeforest.public']
    # download_from = ['spikeforest.kbucket', 'spikeforest.public']

    print('Loading analysis file...')
    analysis_obj = mt.loadObject(path=args.analysis_file)
    if not analysis_obj:
        raise Exception('Unable to load analysis file: {}'.format(args.analysis_file))

    job_timeout = args.job_timeout
    if job_timeout is not None:
        job_timeout = int(job_timeout)

    skip_failing = args.skip_failing
    skip_timed_out = args.skip_timed_out

    use_slurm = args.use_slurm

    analysis_names = args.analyses.split(',')

    sorter_definitions = analysis_obj['sorters']
    analysis_definitions = analysis_obj['analyses']

    # Realize the containers for the sorters
    for sorter in sorter_definitions.values():
        sorter_name = sorter['name']
        sorting_params = sorter['params']
        processor_name = sorter['processor_name']
        SORTER, CONTAINER = sa.find_sorter_processor_and_container(processor_name)
        print('Checking container for SORTER: {}'.format(sorter_name))
        if CONTAINER:
            print('Realizing container for sorter: {}'.format(CONTAINER))
            mt.realizeFile(path=CONTAINER)

    # Loop through all the analyses
    for aname in analysis_names:
        if aname not in analysis_definitions:
            raise Exception('Analysis {} not found in analysis file'.format(aname))
        A = analysis_definitions[aname]
        recordings = A['recordings']
        output_path = A['output']
        sorter_keys = A['sorter_keys']
        sorters = [sorter_definitions[skey] for skey in sorter_keys]
        sorters = _expand_sorters(sorters)

        # Grab the recordings
        print('Loading the recordings for {}...'.format(aname))
        recordings = []
        studies = []
        study_sets = []
        for recordings_path in A['recordings']:
            obj = mt.loadObject(path=recordings_path)
            if obj is None:
                raise Exception('Unable to load recordings from: ' + recordings_path)
            recordings = recordings + obj['recordings']
            studies = studies + obj['studies']
            study_sets = study_sets + obj['study_sets']
        print('Found {} recordings in {} studies'.format(len(recordings), len(studies)))

        print('Using output path: {}'.format(output_path))

        print('Using sorters: ', [sorter['name'] for sorter in sorters])

        print('Using job timeout (sec): {}'.format(job_timeout))

        if args.test:
            print('Test mode... so only retaining a couple recordings and one study and not storing output.')
            recordings = recordings[0:2]
            studies = studies[0:1]
            output_path = None

        # Set up slurm configuration
        slurm_working_dir = 'tmp_slurm_job_handler_' + _random_string(5)
        job_handler = mlpr.SlurmJobHandler(
            working_dir=slurm_working_dir
        )
        if use_slurm:
            job_handler.addBatchType(
                name='cpu',
                num_workers_per_batch=14,
                num_cores_per_job=2,
                time_limit_per_batch=job_timeout * 3,
                use_slurm=True,
                max_simultaneous_batches=20,
                additional_srun_opts=['-p ccm']
            )
            job_handler.addBatchType(
                name='gpu',
                num_workers_per_batch=4,
                num_cores_per_job=2,
                time_limit_per_batch=job_timeout * 3,
                use_slurm=True,
                max_simultaneous_batches=20,
                additional_srun_opts=['-p gpu', '--gres=gpu:v100-32gb:1']
                # additional_srun_opts=['-p gpu', '--gres=gpu:v100-32gb:1']
            )
        else:
            job_handler.addBatchType(
                name='cpu',
                num_workers_per_batch=multiprocessing.cpu_count(),
                num_cores_per_job=2,
                max_simultaneous_batches=1,
                use_slurm=False
            )
            job_handler.addBatchType(
                name='gpu',
                num_workers_per_batch=1,
                num_cores_per_job=2,
                max_simultaneous_batches=1,
                use_slurm=False
            )

        # Start the job queue
        with mlpr.JobQueue(job_handler=job_handler) as JQ:
            # Summarize the recordings
            mtlogging.sublog('summarize-recordings')
            jobs_info = sa.ComputeRecordingInfo.createJobs([
                dict(
                    recording_dir=recording['directory'],
                    channels=recording.get('channels', []),
                    json_out={'ext': '.json', 'upload': True},
                    _timeout=job_timeout,
                    _container='default',
                    _label='Summarize recording: ' + recording.get('name', ''),
                    _compute_requirements=dict(batch_type='cpu')
                )
                for recording in recordings
            ])
            jobs_units_info = sa.ComputeUnitsInfo.createJobs([
                dict(
                    recording_dir=recording['directory'],
                    firings=recording['directory'] + '/firings_true.mda',
                    unit_ids=recording.get('units_true', None),
                    channel_ids=recording.get('channels', None),
                    json_out={'ext': '.json', 'upload': True},
                    _timeout=job_timeout,
                    _container='default',
                    _label='Compute units info for recording: ' + recording.get('name', ''),
                    _compute_requirements=dict(batch_type='cpu')
                )
                for recording in recordings
            ])
            for i, recording in enumerate(recordings):
                recording['results'] = dict()
                recording['results']['info'] = jobs_info[i].execute()  # sends the job to the queue
                recording['results']['units_info'] = jobs_units_info[i].execute()  # sends the job to the queue
                recording['results']['sorting'] = dict()
                recording['results']['comparison'] = dict()
                recording['results']['sorted_units_info'] = dict()

            # Sort the recordings
            mtlogging.sublog('summarize-recordings')
            for sorter in sorters:
                sorter_name = sorter['name']
                sorting_params = sorter['params']
                processor_name = sorter['processor_name']
                SORTER, CONTAINER = sa.find_sorter_processor_and_container(processor_name)
                if sorter.get('gpu', False):
                    batch_type = 'gpu'
                else:
                    batch_type = 'cpu'
                sorting_jobs = SORTER.createJobs([
                    dict(
                        _container=CONTAINER,
                        _timeout=job_timeout,
                        _label='Sort recording {}/{} using {}'.format(recording.get('study', ''), recording.get('name', ''), sorter.get('name', '')),
                        _additional_files_to_realize=[recording['directory'] + '/raw.mda'],
                        recording_dir=recording['directory'],
                        channels=recording.get('channels', []),
                        firings_out=dict(ext='.mda'),
                        _compute_requirements=dict(batch_type=batch_type),
                        _skip_failing=skip_failing,
                        _skip_timed_out=skip_timed_out,
                        _force_run=sorter.get('force_run', False),
                        **sorting_params
                    )
                    for recording in recordings
                ])
                for i, recording in enumerate(recordings):
                    recording['results']['sorting'][sorter_name] = sorting_jobs[i].execute()  # sends job to the queue

                comparison_jobs = sa.GenSortingComparisonTable.createJobs([
                    dict(
                        firings=recording['results']['sorting'][sorter_name].outputs['firings_out'],
                        firings_true=recording['directory'] + '/firings_true.mda',
                        units_true=recording.get('units', []),
                        json_out={'ext': '.json'},
                        html_out={'ext': '.html'},
                        _timeout=job_timeout * 2,
                        _label='Compare with truth {} {}/{}'.format(sorter_name, recording.get('study', ''), recording.get('name', '')),
                        _container='default',
                        _compute_requirements=dict(batch_type='cpu')
                    )
                    for recording in recordings
                ])
                jobs_sorted_units_info = sa.ComputeUnitsInfo.createJobs([
                    dict(
                        recording_dir=recording['directory'],
                        firings=recording['results']['sorting'][sorter_name].outputs['firings_out'],
                        unit_ids=None,
                        channel_ids=recording.get('channels', None),
                        json_out={'ext': '.json', 'upload': True},
                        _timeout=job_timeout,
                        _container='default',
                        _label='Compute sorted units info for recording: ' + recording.get('name', ''),
                        _compute_requirements=dict(batch_type='cpu')
                    )
                    for recording in recordings
                ])
                for i, recording in enumerate(recordings):
                    recording['results']['comparison'][sorter_name] = comparison_jobs[i].execute()  # sends job to the queue
                    recording['results']['sorted_units_info'][sorter_name] = jobs_sorted_units_info[i].execute()  # sends job to the queue

            # wait for all jobs to complete
            JQ.wait()

            for recording in recordings:
                recording['summary'] = dict(
                    plots=dict(),
                    computed_info=mt.loadObject(path=recording['results']['info'].outputs['json_out']),
                    true_units_info=recording['results']['units_info'].outputs['json_out']
                )
                if upload_to:
                    # upload true units info into kachery
                    mt.createSnapshot(path=recording['summary']['true_units_info'], upload_to=upload_to)

            sorting_results = []
            for sorter in sorters:
                print('Collecting results for {} {}'.format(aname, sorter['name']))
                sorter_name = sorter['name']
                sorting_params = sorter['params']
                processor_name = sorter['processor_name']
                SORTER, CONTAINER = sa.find_sorter_processor_and_container(processor_name)
                for recording in recordings:
                    sorting_result = recording['results']['sorting'][sorter_name]
                    comparison_result = recording['results']['comparison'][sorter_name]
                    sorted_units_info = recording['results']['sorted_units_info'][sorter_name]
                    sr = dict(
                        recording=recording,
                        sorter=sorter,
                        firings_true=recording['directory'] + '/firings_true.mda',
                        processor_name=processor_name,
                        processor_version=SORTER.VERSION,
                        execution_stats=sorting_result.runtime_info,
                        console_out=sorting_result.console_out,
                        container=CONTAINER,
                    )
                    if upload_to:
                        # upload console output to kachery
                        mt.createSnapshot(path=sr['console_out'], upload_to=upload_to)
                    if sorting_result.retcode == 0:
                        sr['firings'] = sorting_result.outputs['firings_out']
                        sr['comparison_with_truth'] = dict(
                            json=comparison_result.outputs['json_out'],
                            html=comparison_result.outputs['html_out']
                        )
                        sr['sorted_units_info'] = sorted_units_info.outputs['json_out']
                        if upload_to:
                            # upload firings, comparison, and sorted units info to kachery
                            mt.createSnapshot(path=sr['firings'], upload_to=upload_to)
                            mt.createSnapshot(path=sr['comparison_with_truth']['json'], upload_to=upload_to)
                            mt.createSnapshot(path=sr['sorted_units_info'], upload_to=upload_to)
                    else:
                        sr['firings'] = None
                        sr['comparison_with_truth'] = None
                        sr['sorted_units_info'] = None
                    sorting_results.append(sr)

            for recording in recordings:
                del recording['results']

            # Aggregate the results
            mtlogging.sublog('aggregate')
            aggregated_sorting_results = sa.aggregate_sorting_results(
                studies, recordings, sorting_results)

            output_object = dict(
                studies=studies,
                recordings=recordings,
                study_sets=study_sets,
                sorting_results=sorting_results,
                aggregated_sorting_results=mt.saveObject(
                    object=aggregated_sorting_results, upload_to=upload_to)
            )

            if output_path:
                print('Saving the output to {}'.format(output_path))
                mtlogging.sublog('save-output-path')
                address = mt.saveObject(output_object, upload_to=upload_to)
                if not address:
                    raise Exception('Problem saving output object.')
                if not mt.createSnapshot(path=address, dest_path=output_path):
                    raise Exception('Problem saving output to {}'.format(output_path))

            mtlogging.sublog('show-output-summary')
            for sr in aggregated_sorting_results['study_sorting_results']:
                study_name = sr['study']
                sorter_name = sr['sorter']
                n1 = np.array(sr['num_matches'])
                n2 = np.array(sr['num_false_positives'])
                n3 = np.array(sr['num_false_negatives'])
                accuracies = n1 / (n1 + n2 + n3)
                avg_accuracy = np.mean(accuracies)
                txt = 'STUDY: {}, SORTER: {}, AVG ACCURACY: {}'.format(study_name, sorter_name, avg_accuracy)
                print(txt)


def _expand_sorters(sorters):
    ret = []
    for sorter in sorters:
        a = _expand_sorter(sorter)
        ret = ret + a
    return ret


def _expand_sorter(sorter):
    params = sorter['params']
    for pname, pval in params.items():
        if type(pval) == dict:
            if '_list' in pval:
                plist = pval['_list']
            elif '_range' in pval:
                plist = list(range(pval['_range'][0], pval['_range'][1], pval['_range'][2]))
            else:
                plist = None
            if plist:
                ret = []
                for ii, lval in enumerate(plist):
                    sorter2 = deepcopy(sorter)
                    sorter2['params'][pname] = lval
                    sorter2['name'] = sorter2['name'] + '-{}'.format(ii)
                    ret = ret + _expand_sorter(sorter2)
                return ret
    return [sorter]


def _random_string(num_chars):
    chars = 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789'
    return ''.join(random.choice(chars) for _ in range(num_chars))


if __name__ == "__main__":
    main()
